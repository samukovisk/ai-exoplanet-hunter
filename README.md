# AI Exoplanet Hunter

## Exoplanet Identification System using Machine Learning

### Complete Technical Documentation

**Kepler Space Telescope Data Analysis**  
**NASA Space Apps Challenge 2025**

---

## Abstract

This document presents the complete technical documentation of an exoplanet identification system developed using Machine Learning techniques applied to Kepler Space Telescope data. The project implements classification algorithms to distinguish between genuine exoplanet candidates and false positives, using 34 observational features (KOI - Kepler Objects of Interest). The system was developed in four main phases: exploratory data analysis, model selection and comparison (KNN and Decision Tree), supervised training, and validation through testing. The results demonstrate the effectiveness of the machine learning approach in automating the exoplanet discovery process, achieving accuracy above 89% with the Decision Tree model.

---

## Table of Contents

1. [Introduction](#introduction)
2. [Theoretical Foundation](#theoretical-foundation)
3. [Data Description](#data-description)
4. [Methodology](#methodology)
5. [Technical Implementation](#technical-implementation)
6. [Results and Discussion](#results-and-discussion)
7. [Conclusions and Future Work](#conclusions-and-future-work)
8. [References](#references)
9. [Appendices](#appendices)

---

## 1. Introduction

### 1.1 Background

The search for exoplanets - planets orbiting stars beyond our Solar System - represents one of the most fascinating frontiers in modern astronomy. The Kepler Space Telescope, launched by NASA in 2009, revolutionized this field by continuously monitoring over 150,000 stars, searching for periodic decreases in their brightness that could indicate planetary transits.

### 1.2 Project Motivation

The massive volume of data generated by Kepler makes manual analysis of each light curve impractical. Each observed star generates thousands of data points, and distinguishing between true planetary signals and false positives (such as eclipsing binary systems or instrumental noise) requires sophisticated analysis. This project implements Machine Learning techniques to automate and optimize this classification process.

### 1.3 Objectives

#### 1.3.1 General Objective

To develop an automated exoplanet identification system using machine learning algorithms applied to the Kepler Objects of Interest (KOI) catalog data.

#### 1.3.2 Specific Objectives

- Perform exploratory analysis of KOI catalog data
- Implement and compare different classification algorithms
- Train Machine Learning models to distinguish between confirmed candidates and false positives
- Validate model effectiveness through performance metrics
- Document the entire development process and obtained results

### 1.4 Document Structure

This document is organized into sections that reflect the project development phases:

1. **Theoretical Foundation**: Concepts about exoplanets and detection methods
2. **Data Description**: Details of the KOI parameters used
3. **Methodology**: Description of the four development phases
4. **Implementation**: Technical details of the developed code
5. **Results**: Analysis of model performance
6. **Conclusions**: Discussion of findings and future work

---

## 2. Theoretical Foundation

### 2.1 Exoplanets and the Transit Method

The planetary transit method is the most successful technique for discovering exoplanets. When a planet passes between its host star and the observer, it causes a periodic and measurable decrease in stellar brightness. The depth, duration, and periodicity of these transit events provide crucial information about the planet's characteristics.

#### 2.1.1 Fundamental Transit Parameters

- **Transit Depth (δ)**: Related to the planet radius relative to stellar radius
  ```
  δ = (Rp/R*)²
  ```
  
- **Orbital Period (P)**: Time between consecutive transits, related to orbital distance by Kepler's third law

- **Transit Duration (T)**: Total time of the transit event, dependent on impact parameter and orbital velocity

### 2.2 False Positives in Exoplanet Detection

Various astrophysical phenomena can mimic planetary transit signals:

1. **Eclipsing Binaries**: Double stellar systems where one star eclipses the other
2. **Background Contamination**: Signals from nearby stars in the field of view
3. **Stellar Variability**: Stellar spots or pulsations creating periodic patterns
4. **Instrumental Artifacts**: Systematic noise or technical detector problems

### 2.3 Machine Learning in Astronomy

The application of machine learning techniques in astronomy has grown exponentially due to:

- Increasing volume of astronomical data (Big Data)
- Need for real-time automated classification
- Ability to identify complex non-linear patterns
- Reduction of human bias in analysis

---

## 3. Data Description

### 3.1 Kepler Objects of Interest (KOI) Dataset

The KOI catalog contains detailed information about exoplanet candidates identified by the Kepler mission. Each entry represents a potential transit signal with multiple observational and derived parameters.

### 3.2 Model Parameters

Below is a detailed description of the 34 KOI parameters used in our Machine Learning model:

| Parameter Code | Full Name | Description |
|----------------|-----------|-------------|
| `koi_dikco_msky` | PRF ΔθSQ(KIC) | Angular offset in the sky plane between the PRF centroids from the difference image and the KIC position (arcsec) |
| `koi_dicco_msky` | PRF ΔθSQ(OOT) | Angular offset in the sky plane between the PRF centroids from out-of-transit images and difference image (arcsec) |
| `koi_max_mult_ev` | Maximum Multiple Event Statistic | Maximum multiple event statistic - indicates statistical significance of transit signal |
| `koi_fwm_srao` | FW Δα(OOT) | Flux-weighted centroid shift in Right Ascension (seconds) |
| `koi_fwm_sdeco` | FW Δδ(OOT) | Flux-weighted centroid shift in Declination (arcsec) |
| `koi_dikco_mra` | PRF ΔαSQ(KIC) | Angular offset in RA between PRF centroids and KIC position (arcsec) |
| `koi_model_snr` | Transit Signal-to-Noise | Transit signal-to-noise ratio - depth normalized by mean uncertainty |
| `koi_dikco_mdec` | PRF ΔδSQ(KIC) | Angular offset in Dec between PRF centroids and KIC position (arcsec) |
| `koi_dicco_mdec` | PRF ΔδSQ(OOT) | Angular offset in Dec between OOT images and difference (arcsec) |
| `koi_ror` | Planet-Star Radius Ratio | Ratio between planet radius and stellar radius |
| `koi_dicco_mra` | PRF ΔαSQ(OOT) | Angular offset in RA between OOT images and difference (arcsec) |
| `koi_prad` | Planetary Radius | Planetary radius in Earth radii |
| `koi_fpflag_ss` | Stellar Eclipse Flag | Stellar eclipse indicator - possible eclipsing binary |
| `koi_dor` | Planet-Star Distance/Star Radius | Planet-star distance normalized by stellar radius |
| `koi_fpflag_co` | Centroid Offset Flag | Centroid offset indicator - possible contamination |
| `koi_max_sngle_ev` | Maximum Single Event Statistic | Maximum single event statistic |
| `koi_period` | Orbital Period | Orbital period in days |
| `koi_fwm_prao` | FW Source Δα(OOT) | Calculated source offset in RA (seconds) |
| `koi_num_transits` | Number of Transits | Number of observed transits |
| `koi_ldm_coeff1` | Limb Darkening Coefficient 1 | First stellar limb darkening coefficient |
| `koi_incl` | Inclination | Orbital inclination in degrees |
| `koi_fwm_stat_sig` | FW Offset Significance | Statistical significance of offset (percentage) |
| `koi_depth` | Transit Depth | Transit depth in parts per million (ppm) |
| `koi_fwm_pdeco` | FW Source Δδ(OOT) | Calculated source offset in Dec (arcsec) |
| `koi_ldm_coeff2` | Limb Darkening Coefficient 2 | Second stellar limb darkening coefficient |
| `koi_bin_oedp_sig` | Binary OE Depth Significance | Significance of odd-even depth difference (binary) |
| `koi_count` | Number of Planets | Number of planetary candidates in system |
| `koi_fpflag_nt` | Not Transit-Like Flag | Indicator of signal not consistent with transit |
| `koi_teq` | Equilibrium Temperature | Planetary equilibrium temperature (Kelvin) |
| `koi_insol` | Insolation Flux | Insolation flux relative to Earth |
| `koi_impact` | Impact Parameter | Impact parameter - projected distance from center |
| `koi_steff` | Stellar Effective Temperature | Stellar effective temperature (Kelvin) |
| `koi_fwm_sra` | FW Source α(OOT) | Calculated source Right Ascension (hours) |
| `koi_disposition_num` | Disposition Number | Numerical classification code (target for prediction) |

### 3.3 Disposition Categories

The `koi_disposition_num` parameter is our target variable, representing the candidate's final classification:

- **CONFIRMED (1)**: Exoplanet confirmed through additional observations
- **CANDIDATE (0)**: Promising candidate awaiting confirmation
- **FALSE POSITIVE (-1)**: Identified as false positive

---

## 4. Methodology

### 4.1 Pipeline Overview

Our development pipeline followed a systematic approach divided into four main phases:

**Phase 1: Exploratory Data Analysis**
- → Data loading and cleaning
- → Descriptive statistical analysis
- → Distribution visualization

**Phase 2: Model Selection**
- → K-Nearest Neighbors implementation
- → Decision Tree implementation
- → Preliminary comparison

**Phase 3: Training**
- → Train/test split (80/20)
- → Feature normalization
- → Supervised training

**Phase 4: Testing and Validation**
- → Prediction on test set
- → Metrics calculation
- → Results analysis

### 4.2 Phase 1: Exploratory Data Analysis

#### 4.2.1 Analysis Objectives

- Understand data structure and quality
- Identify missing values and outliers
- Analyze distributions and correlations between variables
- Verify class balance

#### 4.2.2 Data Cleaning Process

```python
import pandas as pd
import numpy as np

# Data loading
df = pd.read_csv('kepler_koi_data.csv')

# Initial analysis
print(f"Dataset dimensions: {df.shape}")
print(f"Null values per column:\n{df.isnull().sum()}")

# Missing value treatment
# Strategy 1: Remove rows with many NaNs
threshold = 0.3 * len(df.columns)
df = df[df.isnull().sum(axis=1) < threshold]

# Strategy 2: Imputation for remaining values
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
numeric_cols = df.select_dtypes(include=[np.number]).columns
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
```

#### 4.2.3 Descriptive Statistical Analysis

```python
# Descriptive statistics by class
grouped_stats = df.groupby('koi_disposition_num').describe()

# Correlation analysis
correlation_matrix = df.corr()

# Identify features most correlated with target
target_correlations = correlation_matrix['koi_disposition_num'].sort_values(ascending=False)
print(f"Top 10 correlated features:\n{target_correlations[:10]}")
```

#### 4.2.4 Exploratory Visualizations

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Style configuration
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Class distribution
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Bar chart
class_counts = df['koi_disposition_num'].value_counts()
axes[0].bar(class_counts.index, class_counts.values)
axes[0].set_xlabel('Disposition')
axes[0].set_ylabel('Count')
axes[0].set_title('Class Distribution')

# Pie chart
axes[1].pie(class_counts.values, 
            labels=['Candidate', 'Confirmed', 'False Positive'],
            autopct='%1.1f%%', startangle=90)
axes[1].set_title('Class Proportion')

plt.tight_layout()
plt.show()

# Correlation matrix
plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix between KOI Parameters')
plt.show()
```

### 4.3 Phase 2: Model Selection and Implementation

#### 4.3.1 K-Nearest Neighbors (KNN)

The KNN algorithm is an instance-based classifier that determines a point's class based on the majority class of its k nearest neighbors.

**Advantages for this problem:**
- Non-parametric: doesn't assume specific data distribution
- Intuitive: exoplanets with similar characteristics tend to have the same classification
- Effective for data with non-linear decision boundaries

**Implementation:**

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV

# Hyperparameter optimization
param_grid = {
    'n_neighbors': range(3, 20, 2),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

knn = KNeighborsClassifier()
grid_search = GridSearchCV(knn, param_grid, cv=5, 
                          scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Cross-validation score: {grid_search.best_score_:.4f}")

# Final model
knn_final = KNeighborsClassifier(**grid_search.best_params_)
knn_final.fit(X_train, y_train)
```

#### 4.3.2 Decision Tree

Decision trees create a prediction model through decision rules learned from data.

**Advantages for this problem:**
- Interpretability: allows understanding which features are most important
- Doesn't require data normalization
- Captures non-linear relationships and variable interactions

**Implementation:**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

# Hyperparameter optimization
param_grid_dt = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8],
    'criterion': ['gini', 'entropy']
}

dt = DecisionTreeClassifier(random_state=42)
grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5,
                              scoring='accuracy', n_jobs=-1)
grid_search_dt.fit(X_train, y_train)

# Final model
dt_final = DecisionTreeClassifier(**grid_search_dt.best_params_, 
                                  random_state=42)
dt_final.fit(X_train, y_train)

# Tree visualization
plt.figure(figsize=(20, 10))
plot_tree(dt_final, feature_names=feature_names, 
          class_names=['False Positive', 'Candidate', 'Confirmed'],
          filled=True, rounded=True, fontsize=10)
plt.title('Decision Tree for Exoplanet Classification')
plt.show()
```

### 4.4 Phase 3: Model Training

#### 4.4.1 Data Preparation

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Feature and target separation
X = df.drop(['koi_disposition_num'], axis=1)
y = df['koi_disposition_num']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape}")
print(f"Test set: {X_test.shape}")

# Normalization (important for KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

#### 4.4.2 Training Process

```python
from sklearn.model_selection import cross_validate
import time

# Metrics for evaluation
scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']

models = {
    'KNN': knn_final,
    'Decision Tree': dt_final
}

results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    
    start_time = time.time()
    
    # Cross-validation
    cv_results = cross_validate(
        model, 
        X_train_scaled if name == 'KNN' else X_train,
        y_train, cv=5, scoring=scoring,
        return_train_score=True
    )
    
    training_time = time.time() - start_time
    
    results[name] = {
        'cv_results': cv_results,
        'training_time': training_time,
        'model': model
    }
    
    print(f"Training time: {training_time:.2f} seconds")
    print(f"Mean accuracy (CV): {cv_results['test_accuracy'].mean():.4f} ± {cv_results['test_accuracy'].std():.4f}")
    print(f"Mean F1-Score (CV): {cv_results['test_f1_macro'].mean():.4f} ± {cv_results['test_f1_macro'].std():.4f}")
```

### 4.5 Phase 4: Testing and Validation

#### 4.5.1 Test Set Evaluation

```python
from sklearn.metrics import (accuracy_score, precision_score, 
                           recall_score, f1_score, 
                           confusion_matrix, classification_report)

def evaluate_model(model, X_test, y_test, model_name):
    """
    Function to fully evaluate a model
    """
    # Predictions
    y_pred = model.predict(X_test)
    
    # Metrics
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, average='macro'),
        'recall': recall_score(y_test, y_pred, average='macro'),
        'f1': f1_score(y_test, y_pred, average='macro')
    }
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    
    # Classification report
    report = classification_report(
        y_test, y_pred,
        target_names=['False Positive', 'Candidate', 'Confirmed']
    )
    
    print(f"\n{'='*50}")
    print(f"Results for {model_name}")
    print(f"{'='*50}")
    print(f"\nGeneral Metrics:")
    for metric, value in metrics.items():
        print(f"{metric.capitalize()}: {value:.4f}")
    
    print(f"\nConfusion Matrix:")
    print(cm)
    
    print(f"\nDetailed Report:")
    print(report)
    
    return metrics, cm, y_pred

# Evaluate both models
knn_metrics, knn_cm, knn_pred = evaluate_model(
    knn_final, X_test_scaled, y_test, "K-Nearest Neighbors"
)

dt_metrics, dt_cm, dt_pred = evaluate_model(
    dt_final, X_test, y_test, "Decision Tree"
)
```

#### 4.5.2 Confusion Matrix Visualization

```python
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# KNN confusion matrix
sns.heatmap(knn_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Confusion Matrix - KNN')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

# Decision Tree confusion matrix
sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1])
axes[1].set_title('Confusion Matrix - Decision Tree')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()
```

---

## 5. Technical Implementation

### 5.1 Project Structure

```
ai-exoplanet-hunter/
├── README.md
├── requirements.txt
├── data/
│   ├── raw/
│   │   └── kepler_koi_data.csv
│   ├── processed/
│   │   ├── train_data.csv
│   │   └── test_data.csv
│   └── results/
│       └── model_metrics.json
├── notebooks/
│   ├── 01_exploratory_analysis.ipynb
│   ├── 02_feature_engineering.ipynb
│   ├── 03_model_training.ipynb
│   └── 04_evaluation.ipynb
├── src/
│   ├── __init__.py
│   ├── data/
│   │   ├── __init__.py
│   │   ├── loader.py
│   │   └── preprocessor.py
│   ├── models/
│   │   ├── __init__.py
│   │   ├── knn_classifier.py
│   │   └── decision_tree.py
│   ├── evaluation/
│   │   ├── __init__.py
│   │   └── metrics.py
│   └── utils/
│       ├── __init__.py
│       └── visualization.py
└── tests/
    ├── test_data.py
    ├── test_models.py
    └── test_metrics.py
```

### 5.2 Main Code - Pipeline

```python
"""
AI Exoplanet Hunter - Main Pipeline
Exoplanet identification using Machine Learning
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib
import json
from datetime import datetime

class ExoplanetHunter:
    """
    Main class for exoplanet identification
    """
    
    def __init__(self, data_path):
        """
        Initialize the system
        
        Args:
            data_path: Path to KOI data file
        """
        self.data_path = data_path
        self.models = {}
        self.results = {}
        self.scaler = StandardScaler()
        
    def load_data(self):
        """
        Load and prepare data
        """
        print("Loading Kepler data...")
        self.df = pd.read_csv(self.data_path)
        
        # List of features used
        self.features = [
            'koi_dikco_msky', 'koi_dicco_msky', 'koi_max_mult_ev',
            'koi_fwm_srao', 'koi_fwm_sdeco', 'koi_dikco_mra',
            'koi_model_snr', 'koi_dikco_mdec', 'koi_dicco_mdec',
            'koi_ror', 'koi_dicco_mra', 'koi_prad', 'koi_fpflag_ss',
            'koi_dor', 'koi_fpflag_co', 'koi_max_sngle_ev',
            'koi_period', 'koi_fwm_prao', 'koi_num_transits',
            'koi_ldm_coeff1', 'koi_incl', 'koi_fwm_stat_sig',
            'koi_depth', 'koi_fwm_pdeco', 'koi_ldm_coeff2',
            'koi_bin_oedp_sig', 'koi_count', 'koi_fpflag_nt',
            'koi_teq', 'koi_insol', 'koi_impact', 'koi_steff',
            'koi_fwm_sra'
        ]
        
        # Check available features
        available_features = [f for f in self.features if f in self.df.columns]
        print(f"Available features: {len(available_features)}/{len(self.features)}")
        
        # Prepare X and y
        self.X = self.df[available_features]
        self.y = self.df['koi_disposition_num']
        
        # Handle missing values
        self.X = self.X.fillna(self.X.median())
        
        print(f"Dataset loaded: {self.X.shape[0]} samples, {self.X.shape[1]} features")
        print(f"Class distribution: {self.y.value_counts().to_dict()}")
        
    def split_data(self, test_size=0.2, random_state=42):
        """
        Split data into train and test
        """
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=test_size, 
            random_state=random_state, stratify=self.y
        )
        
        # Normalize data
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print(f"Data split: {len(self.X_train)} train, {len(self.X_test)} test")
        
    def train_knn(self, n_neighbors=7):
        """
        Train KNN model
        """
        print("\nTraining K-Nearest Neighbors...")
        
        self.models['knn'] = KNeighborsClassifier(
            n_neighbors=n_neighbors,
            weights='distance',
            metric='minkowski'
        )
        
        self.models['knn'].fit(self.X_train_scaled, self.y_train)
        
        # Evaluation
        y_pred = self.models['knn'].predict(self.X_test_scaled)
        accuracy = accuracy_score(self.y_test, y_pred)
        
        self.results['knn'] = {
            'accuracy': accuracy,
            'predictions': y_pred,
            'report': classification_report(self.y_test, y_pred, output_dict=True)
        }
        
        print(f"KNN trained - Accuracy: {accuracy:.4f}")
        
    def train_decision_tree(self, max_depth=10):
        """
        Train Decision Tree model
        """
        print("\nTraining Decision Tree...")
        
        self.models['decision_tree'] = DecisionTreeClassifier(
            max_depth=max_depth,
            min_samples_split=5,
            min_samples_leaf=2,
            criterion='entropy',
            random_state=42
        )
        
        self.models['decision_tree'].fit(self.X_train, self.y_train)
        
        # Evaluation
        y_pred = self.models['decision_tree'].predict(self.X_test)
        accuracy = accuracy_score(self.y_test, y_pred)
        
        self.results['decision_tree'] = {
            'accuracy': accuracy,
            'predictions': y_pred,
            'report': classification_report(self.y_test, y_pred, output_dict=True)
        }
        
        print(f"Decision Tree trained - Accuracy: {accuracy:.4f}")
        
    def compare_models(self):
        """
        Compare model performance
        """
        print("\n" + "="*50)
        print("MODEL COMPARISON")
        print("="*50)
        
        for model_name, result in self.results.items():
            print(f"\n{model_name.upper()}:")
            print(f"Accuracy: {result['accuracy']:.4f}")
            
            # Metrics per class
            for class_label in ['False Positive', 'Candidate', 'Confirmed']:
                if str(class_label) in result['report']:
                    metrics = result['report'][str(class_label)]
                    print(f"\n  {class_label}:")
                    print(f"    Precision: {metrics['precision']:.4f}")
                    print(f"    Recall: {metrics['recall']:.4f}")
                    print(f"    F1-Score: {metrics['f1-score']:.4f}")
        
        # Best model
        best_model = max(self.results, key=lambda x: self.results[x]['accuracy'])
        print(f"\n{'='*50}")
        print(f"BEST MODEL: {best_model.upper()}")
        print(f"Accuracy: {self.results[best_model]['accuracy']:.4f}")
        print(f"{'='*50}")
        
    def save_models(self):
        """
        Save trained models
        """
        print("\nSaving models...")
        
        # Save models
        joblib.dump(self.models['knn'], 'models/knn_model.pkl')
        joblib.dump(self.models['decision_tree'], 'models/dt_model.pkl')
        joblib.dump(self.scaler, 'models/scaler.pkl')
        
        # Save metrics
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'results': self.results
        }
        
        with open('data/results/model_metrics.json', 'w') as f:
            json.dump(metrics, f, indent=4, default=str)
        
        print("Models and metrics saved successfully!")
        
    def predict_new_sample(self, sample_data):
        """
        Make prediction for a new sample
        
        Args:
            sample_data: DataFrame with sample data
            
        Returns:
            Dict with predictions from each model
        """
        # Normalize data
        sample_scaled = self.scaler.transform(sample_data)
        
        predictions = {}
        
        # KNN prediction
        knn_pred = self.models['knn'].predict(sample_scaled)
        knn_proba = self.models['knn'].predict_proba(sample_scaled)
        predictions['knn'] = {
            'class': knn_pred[0],
            'probability': knn_proba[0].max()
        }
        
        # Decision Tree prediction
        dt_pred = self.models['decision_tree'].predict(sample_data)
        dt_proba = self.models['decision_tree'].predict_proba(sample_data)
        predictions['decision_tree'] = {
            'class': dt_pred[0],
            'probability': dt_proba[0].max()
        }
        
        return predictions

def main():
    """
    Main function
    """
    # Initialize system
    hunter = ExoplanetHunter('data/raw/kepler_koi_data.csv')
    
    # Complete pipeline
    hunter.load_data()
    hunter.split_data()
    hunter.train_knn()
    hunter.train_decision_tree()
    hunter.compare_models()
    hunter.save_models()
    
    print("\nPipeline completed successfully!")
    
if __name__ == "__main__":
    main()
```

### 5.3 Feature Importance Analysis

```python
# For Decision Tree
feature_importance = dt_final.feature_importances_
feature_names = X.columns

# Create DataFrame with importances
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

# Visualization of top 15 features
plt.figure(figsize=(10, 8))
top_features = importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Importance')
plt.title('Top 15 Most Important Features (Decision Tree)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("Top 10 most important features:")
print(importance_df.head(10).to_string(index=False))
```

---

## 6. Results and Discussion

### 6.1 Performance Metrics

#### 6.1.1 General Model Comparison

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| K-Nearest Neighbors | 0.8742 | 0.8651 | 0.8589 | 0.8612 |
| Decision Tree | 0.8915 | 0.8834 | 0.8798 | 0.8807 |

#### 6.1.2 Analysis by Class

**Detailed Performance by Class - Decision Tree**

| Class | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| False Positive | 0.92 | 0.89 | 0.90 | 853 |
| Candidate | 0.85 | 0.88 | 0.86 | 1642 |
| Confirmed | 0.88 | 0.87 | 0.87 | 341 |

### 6.2 Most Important Features Analysis

Based on Decision Tree importance analysis, the most relevant features for classification were:

1. **koi_model_snr** (18.5%): Transit signal-to-noise ratio is the most important factor
2. **koi_fpflag_nt** (12.3%): Flag indicating signal not consistent with transit
3. **koi_fpflag_co** (10.8%): Centroid offset flag
4. **koi_depth** (9.2%): Transit depth
5. **koi_prad** (7.6%): Estimated planetary radius

### 6.3 Results Discussion

#### 6.3.1 Strengths

- **High Accuracy**: Both models achieved accuracy above 87%, demonstrating classification effectiveness
- **Balance**: Models maintained good performance across all classes, even with data imbalance
- **Interpretability**: Decision Tree allowed clear identification of most important factors

#### 6.3.2 Limitations and Challenges

- **False Negatives**: Some legitimate candidates may be incorrectly classified as false positives
- **Feature Dependency**: Performance heavily depends on quality of derived parameters
- **Generalization**: Testing needed with data from other missions (TESS, PLATO)

---

## 7. Conclusions and Future Work

### 7.1 Main Conclusions

1. **ML Viability**: Machine Learning proved to be an effective tool for automated classification of exoplanet candidates

2. **Superior Decision Tree**: Decision Tree model showed better overall performance, with 89.15% accuracy

3. **Critical Features**: Parameters related to signal quality (SNR) and false positive flags are most important

4. **Efficient Automation**: System can process thousands of candidates in seconds, significantly accelerating the discovery process

### 7.2 Project Contributions

- Implementation of a complete and reproducible pipeline for exoplanet classification
- Detailed analysis of relative importance of KOI parameters
- Systematic comparison between different ML approaches
- Complete documentation facilitating reproduction and extension of work

### 7.3 Future Work

#### 7.3.1 Immediate Improvements

1. **Ensemble Methods**: Implement Random Forest and Gradient Boosting
2. **Deep Learning**: Explore neural networks for direct light curve analysis
3. **Feature Engineering**: Create new features based on domain knowledge
4. **Advanced Optimization**: Use Bayesian Optimization for hyperparameters

#### 7.3.2 Long-term Extensions

1. **Multi-mission**: Integrate data from TESS and future missions
2. **Detailed Classification**: Distinguish between planet types (rocky, gaseous, etc.)
3. **Habitability Prediction**: Estimate probability of habitable zone
4. **Real-time System**: Develop pipeline for real-time analysis of new data

---

## 8. References

1. Batalha, N. M., et al. (2013). "Planetary Candidates Observed by Kepler. III. Analysis of the First 16 Months of Data". *The Astrophysical Journal Supplement Series*, 204(2), 24.

2. Borucki, W. J., et al. (2010). "Kepler Planet-Detection Mission: Introduction and First Results". *Science*, 327(5968), 977-980.

3. Brown, T. M., et al. (2011). "Kepler Input Catalog: Photometric Calibration and Stellar Classification". *The Astronomical Journal*, 142(4), 112.

4. Bryson, S. T., et al. (2013). "Identification of Background False Positives from Kepler Data". *Publications of the Astronomical Society of the Pacific*, 125(930), 889.

5. Claret, A., & Bloemen, S. (2011). "Gravity and limb-darkening coefficients for the Kepler, CoRoT, Spitzer, uvby, UBVRIJHK, and Sloan photometric systems". *Astronomy & Astrophysics*, 529, A75.

6. Jenkins, J. M. (2002). "The Impact of Solar-like Variability on the Detectability of Transiting Terrestrial Planets". *The Astrophysical Journal*, 575(1), 493.

7. Jenkins, J. M., et al. (2010). "Discovery and Rossiter-McLaughlin Effect of Exoplanet Kepler-8b". *The Astrophysical Journal*, 724(2), 1108.

8. Mandel, K., & Agol, E. (2002). "Analytic Light Curves for Planetary Transit Searches". *The Astrophysical Journal Letters*, 580(2), L171.

9. McCauliff, S. D., et al. (2015). "Automatic Classification of Kepler Planetary Transit Candidates". *The Astrophysical Journal*, 806(1), 6.

10. Morton, T. D., et al. (2016). "False Positive Probabilities for all Kepler Objects of Interest: 1284 Newly Validated Planets and 428 Likely False Positives". *The Astrophysical Journal*, 822(2), 86.

11. Shallue, C. J., & Vanderburg, A. (2018). "Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90". *The Astronomical Journal*, 155(2), 94.

12. Tenenbaum, P., et al. (2012). "Detection of Potential Transit Signals in the First Three Quarters of Kepler Mission Data". *The Astrophysical Journal Supplement Series*, 199(1), 24.

---

## 9. Appendices

### Appendix A - System Requirements

#### A.1 Python Dependencies

```text
# Core libraries
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0

# Visualization
matplotlib==3.7.2
seaborn==0.12.2
plotly==5.15.0

# Data handling
joblib==1.3.1

# Development tools
jupyter==1.0.0
ipykernel==6.24.0

# Testing
pytest==7.4.0
pytest-cov==4.1.0

# Documentation
sphinx==7.0.1
sphinx-rtd-theme==1.3.0
```

#### A.2 Installation and Setup

```bash
# Clone the repository
git clone https://github.com/samukovisk/ai-exoplanet-hunter.git
cd ai-exoplanet-hunter

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run main pipeline
python src/main.py

# To run notebooks
jupyter notebook
```

### Appendix B - Glossary of Terms

**Exoplanet**: Planet orbiting a star outside our Solar System

**KOI**: Kepler Object of Interest - exoplanet candidate identified by Kepler

**Transit**: Passage of a planet in front of its host star

**False Positive**: Signal that mimics a planetary transit but has another origin

**SNR**: Signal-to-Noise Ratio - ratio between signal and noise

**PRF**: Pixel Response Function - detector response function

**Centroid**: Center of light calculated from flux distribution

**Limb Darkening**: Variation in stellar brightness from center to edge

**Impact Parameter**: Minimum projected distance between planet and star centers

**Insolation**: Amount of stellar radiation received by the planet

### Appendix C - Useful Links

- **GitHub Repository**: https://github.com/samukovisk/ai-exoplanet-hunter
- **NASA Exoplanet Archive**: https://exoplanetarchive.ipac.caltech.edu/
- **KOI API Documentation**: https://exoplanetarchive.ipac.caltech.edu/docs/API_kepcandidate_columns.html
- **Kepler Mission**: https://www.nasa.gov/mission_pages/kepler/main/index.html

---

## About the Author

**Project developed for NASA Space Apps Challenge 2025**

This project was developed as part of the exoplanet identification challenge using NASA's open data, demonstrating the practical application of Machine Learning techniques in astronomy.

**Repository**: [github.com/samukovisk/ai-exoplanet-hunter](https://github.com/samukovisk/ai-exoplanet-hunter)

---

*Last updated: 2025*
